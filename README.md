# Kenmark ITan Solutions â€“ AI Chatbot ğŸ¤–

[Live Demo Link](https://website-rag-chatbot-kmit.vercel.app/)

An AI-powered virtual assistant built for the official Kenmark ITan Solutions website.  
The chatbot answers user queries about company services, hosting, development, and FAQs using a Retrieval-Augmented Generation (RAG) approach.

---

## ğŸš€ Features

### Chatbot (User Side)
- Floating chat widget with modern UI
- Streaming AI responses (real-time typing)
- Session-based conversation memory
- Intent handling (greetings, thanks, help)
- Graceful fallback for unknown queries
- Auto-scroll and typing indicator
- Dark mode friendly design

### Admin Dashboard
- Upload Excel knowledge files (.xlsx)
- View most asked questions (analytics)
- Visual analytics using charts
- Knowledge sources: Website + Excel
- Clean, responsive admin UI

---

## ğŸ§  Architecture Overview

User â†’ Chat UI â†’ API Route (/api/chat)
â†“
Intent Detection
â†“
Knowledge Retrieval (RAG)
â†“
LLM (Ollama â€“ Local)
â†“
Streamed Response

## ğŸ›  Tech Stack

### Frontend
- Next.js 16 (App Router)
- TypeScript
- Tailwind CSS v4
- Chart.js

### Backend
- Next.js API Routes
- Session-based memory
- Analytics tracking

### AI / LLM
- Ollama (Mistral / LLaMA models)
- Retrieval-Augmented Generation (RAG)

### Data Sources
- Website content (static JSON)
- Excel files (.xlsx)

---

## ğŸ“‚ Project Structure

app/
â”œâ”€ api/
â”‚ â”œâ”€ chat/
â”‚ â””â”€ admin/
â”œâ”€ admin/
â”œâ”€ page.tsx

components/
â””â”€ ChatWidget.tsx

lib/
â”œâ”€ retriever.ts
â”œâ”€ llm.ts
â”œâ”€ analytics.ts
â”œâ”€ sessionMemory.ts
â””â”€ intent.ts

data/
â”œâ”€ website/
â””â”€ knowledge.xlsx


---

## âš™ï¸ Setup Instructions (Local)

### 1ï¸âƒ£ Install dependencies
```bash
npm install
```

### 2ï¸âƒ£ Install & run Ollama
```
ollama run mistral
```
Ollama must be running on http://localhost:11434

### 3ï¸âƒ£ Start the app
```
npm run dev
```
App runs on:
ğŸ‘‰ http://localhost:3000

Admin dashboard:
ğŸ‘‰ http://localhost:3000/admin


## ğŸ“Š Admin Usage

- Upload Excel knowledge files

- Monitor most asked questions

- Analytics update in real-time

- Knowledge updates reflected instantly

## ğŸŒ Deployment

- The application is deployed on Vercel.

- Frontend + API routes hosted on Vercel

- LLM inference runs locally via Ollama

For full cloud deployment, Ollama can be replaced with Groq/OpenRouter APIs.
